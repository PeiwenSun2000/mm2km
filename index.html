<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SpaceVista</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_v2.svg">
  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SpaceVista: Expanding Visual Spatial Reasoning from mm to km</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Peiwen Sun</a>
              <sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Shiqiang Lang</a>
              <sup>3</sup>,
            </span>
            <span class="author-block">
              <a>Dongming Wu</a>
              <sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Yi Ding</a>
              <sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Huadai Liu</a>
              <sup>4</sup>,
            </span>
            <span class="author-block">
              <a>Zhen Ye</a>
              <sup>4</sup>,
            </span>
            <span class="author-block">
              <a>Rui Liu</a>
              <sup>1</sup>,
            </span><br>
            <span class="author-block">
              <a>Jianan Wang</a>
              <sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Xiangyu Yue</a>
              <sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Multimedia Lab, Chinese University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>Astribot,</span>
            <span class="author-block"><sup>3</sup>Beijing University of Posts and Telecommunications,</span><br>
            <span class="author-block"><sup>4</sup>Hong Kong University of Science and Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.10.377/pdf.min.js"></script>
        <style>
            #the-canvas {
              display: block;
              margin: 0 auto 24px;   /* 居中 */
              max-width: none;      /* 可以超过父容器 */
              width: 120%           /* 超过父容器20% */
              height: auto;         /* 保持比例 */
            }
          </style>
          <canvas id="the-canvas" style="display:block; margin:0 auto;"></canvas>
          <script>
            const url = './static/images/teaser_v2.pdf';
            pdfjsLib.getDocument(url).promise.then(function(pdf) {
              pdf.getPage(1).then(function(page) {
                const scale = 1.0;
                const viewport = page.getViewport({scale: scale});
                const canvas = document.getElementById('the-canvas');
                const context = canvas.getContext('2d');
                canvas.height = viewport.height;
                canvas.width = viewport.width;
          
                page.render({canvasContext: context, viewport: viewport});
              });
            });
          </script>
        <div class="content has-text-justified">
          With the current surge in spatial reasoning research, researchers has made significant
          progress in understanding indoor scenes, but still struggle with holistic scenes
          like robotics and autonomous driving. This paper focuses on developing multiscale
          spatial reasoning across diverse scenarios, addressing two key challenges: a)
          limitation of data relying on indoor 3D scans and manual annotations. b) lack of
          cross-scale supervision with fine-grained modeling. In this paper, we introduce a
          unified system that integrates the structured spatial reasoning knowledge system,
          scale-aware multi-expert modeling, and a progressive training paradigm, as the
          <strong>first attempt</strong> to broaden the spatial perception capabilities of MLLMs to the best
          of our knowledge. Using task-specific, expert-driven automated annotation, we
          curated over 40K scenes across five spatial scales, comprising <strong>SpaceVista-1.5M</strong>,
          a dataset of approximately 1.5M spatial QA pairs spanning 19 distinct task types.
          While using experts as support can inject valuable domain knowledge, it cannot
          serve as a reliable evaluation. To address this, we manually recorded, retrieved, and
          constructed a small, precise, benchmark from mm to km for evaluation. Building on
          this, we propose <strong>SpaceVista</strong>, a model that incorporates multimodal inputs beyond video 
          and employs scale as an anchor for process reward, forming a tailored training
          strategy for spatial reasoning. Finally, Extensive evaluations across 8 datasets,
          including the SpaceVista Evaluation Set, demonstrate competitive performance,
          showcasing our model’s strong generalization across multiple scales and scenarios.
          All datasets, models, and evaluation tools will be released.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">From mm To km Dataset: SpaceVista-1.5M</h2>
        <div class="content has-text-justified">
          We introduce <strong>SpaceVista</strong>, a large-scale multi-scope spatial reasoning benchmark that extends beyond the limitations of existing datasets, 
          which are typically constrained by high annotation and scanning costs. Building on over <strong>40K curated scenes</strong> across five spatial scales, 
          we construct <strong>SpaceVista-1.5M</strong>, comprising approximately <strong>1.5M spatial QA pairs</strong> spanning <strong>19 task types</strong>. </p>
          To further enrich spatial diversity, SpaceVista encompasses <strong>15 tasks</strong> across <strong>5 scene categories</strong> and more than <strong>25 subscene types</strong>, 
          supported by expert-driven automated annotation to ensure balanced coverage across scales and contexts.
        </div>
        <figure class="image">
          <img src="./static/images/data_graph.jpg" alt="data_graph">
        </figure>
      </div>
    </div>
  </div>
</section>  

  
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">SpaceVista Model</h2>
        <div class="content has-text-justified">
          <figure class="image" style="margin-bottom:24px;">
            <img src="./static/images/pipeline.png" alt="pipeline">
          </figure>
          SpaceVista ingests a question with images and videos and 3D geometry, encodes them, projects features to a shared space, and fuses them in an 
          LLM through learnable interaction. A LoRA style scale expert with a scale aware router adapts the model to different spatial scales. 
          Training uses reinforcement learning with stepwise rewards, including scale estimation, semantic recognition, result correctness, and format correctness, 
          to align reasoning and final answers.
        </div>
      </div>
    </div>
  </div>
</section> 

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <style>
        /* 右图的外框：你可以随时改宽高 */
        .right-box { width: 480px; height: 360px; }     /* 调整这两个数即可 */
        .right-box img { width:100%; height:100%; object-fit: contain; } /* 不裁切，等比缩放 */
        /* 若想铺满并允许裁切：把 contain 改成 cover，再用 object-position 定位可视区域 */
        /* .right-box img { object-fit: cover; object-position: 50% 40%; } */
      </style>
        <h2 class="title is-3">Experiment</h2>

        <!-- 并排两图：每张图的宽/高可自行修改 -->
        <div class="columns is-centered">
          <div class="column is-narrow">
            <figure class="image" style="margin-bottom:16px;">
              <img src="./static/images/performance.png" alt="experiment-1"
                   style="width:680px; height:460px; object-fit:contain;">
            </figure>
          </div>
          <div class="column is-narrow">
            <figure class="image right-box" style="margin-bottom:16px;">
              <img src="./static/images/ridar_temp.jpg" alt="experiment-2">
            </figure>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            We introduce <strong>SpaceVista</strong>, a large-scale multi-scope spatial reasoning benchmark that extends beyond the limitations of existing datasets,
            which are typically constrained by high annotation and scanning costs. Building on over <strong>40K curated scenes</strong> across five spatial scales,
            we construct <strong>SpaceVista-1.5M</strong>, comprising approximately <strong>1.5M spatial QA pairs</strong> spanning <strong>19 task types</strong>.
          </p>
          <p>
            To further enrich spatial diversity, SpaceVista encompasses <strong>15 tasks</strong> across <strong>5 scene categories</strong> and more than <strong>25 subscene types</strong>,
            supported by expert-driven automated annotation to ensure balanced coverage across scales and contexts.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
          This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> 
          of <a href="https://nerfies.github.io/">Nerfies website</a>, 
          we just ask that you link back to Nerfies page in the footer.
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
